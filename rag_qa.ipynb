{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Query Agent (Dataset Preparation)\n",
        "\n",
        "The primary objective of this project is to develop a Natural Language Query Agent that leverages Large Language Models (LLMs) and open-source vector indexing and storage frameworks to provide concise responses to straightforward queries within a substantial dataset comprising lecture notes and a table detailing LLM architectures. This notebook offers a comprehensive guide to preparing the dataset for use in our final pipeline, facilitating answers to conversational questions.\n",
        "\n",
        "> The data sources utilized for this project encompass the following:\n",
        "\n",
        "- [Stanford LLMs Lecture Notes](https://stanford-cs324.github.io/winter2022/lectures/)\n",
        "\n",
        "- [Awesome LLM Milestone Papers](https://github.com/Hannibal046/Awesome-LLM#milestone-papers)"
      ],
      "metadata": {
        "id": "JBOxIJ_veiyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin by installing the essential libraries and frameworks required to run this project. The following tools are necessary for its proper functionality.\n",
        "\n",
        "- [transformers](https://pypi.org/project/transformers/)\n",
        "- [accelerate](https://pypi.org/project/accelerate/)\n",
        "- [einops](https://pypi.org/project/einops/)\n",
        "- [langchain](https://pypi.org/project/langchain/)\n",
        "- [xformers](https://pypi.org/project/xformers/)\n",
        "- [bitsandbytes](https://pypi.org/project/bitsandbytes/)\n",
        "- [faiss-gpu](https://pypi.org/project/faiss-gpu/)\n",
        "- [sentence_transformers](https://pypi.org/project/sentence-transformers/)\n",
        "- [pypdf](https://pypi.org/project/pypdf/)\n"
      ],
      "metadata": {
        "id": "WuGG9CbahVfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU transformers accelerate einops langchain xformers bitsandbytes faiss-gpu sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8b1iCjn-Eim",
        "outputId": "fef879a4-155b-4b67-8c0c-e9be677a138f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "U21Ll4SDBg_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn0R6wAhA8oe",
        "outputId": "85e8660f-c221-4d8e-c275-b7c10fd2e169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.16.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "import torch\n",
        "import pickle\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "9VI_xKLd-H6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Next, we'll generate the dataset that will be employed in our project. We will utilize the document loaders provided by Langchain, which include:\n",
        "\n",
        "- [WebBaseLoader](https://python.langchain.com/docs/integrations/document_loaders/web_base): WebBaseLoader represents a robust framework designed for extracting text content from HTML web pages and transforming it into a document format suitable for a wide range of downstream tasks.\n",
        "- [PyPDF](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf): PyPDF is a potent framework crafted for the extraction of text from PDF files.\n",
        "\n",
        "\n",
        "We will retrieve links to webpages and PDF files from the following .txt documents:\n",
        "\n",
        "\tema_dataset_web_links.txt\n",
        "\tema_dataset_pdf_links.txt"
      ],
      "metadata": {
        "id": "VY2rhOqpiKFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_path):\n",
        "    lines = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.rstrip('\\n')\n",
        "            lines.append(line)\n",
        "    return lines"
      ],
      "metadata": {
        "id": "A1AWYHn1IVrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf(file_path):\n",
        "    documents_pdf =[]\n",
        "    pdf_links = read_file(file_path)\n",
        "    for link in pdf_links:\n",
        "        loader = PyPDFLoader(link)\n",
        "        pdf = loader.load()\n",
        "        documents_pdf = documents_pdf + pdf\n",
        "    return documents_pdf"
      ],
      "metadata": {
        "id": "QmsH-wloJimx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/ema_dataset_web_links.txt'\n",
        "web_links = read_file(file_path)\n",
        "loader = WebBaseLoader(web_links)\n",
        "documents_web_links = loader.load()"
      ],
      "metadata": {
        "id": "J-sXi_RwIPHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/ema_dataset_pdf_links.txt'\n",
        "documents_pdf = read_pdf(file_path)"
      ],
      "metadata": {
        "id": "mdySANz7Byq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### At the end of the process, we'll preserve the custom dataset as a .pkl file, an integral part of our pipeline."
      ],
      "metadata": {
        "id": "BCKDKU-elJ1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits_p0 = text_splitter.split_documents(documents_web_links)\n",
        "all_splits_p1 = text_splitter.split_documents(documents_pdf)\n",
        "\n",
        "all_splits = all_splits_p0 + all_splits_p1\n",
        "\n",
        "with open('/content/ema_dataset.pkl', 'wb') as file:\n",
        "    pickle.dump(all_splits, file)"
      ],
      "metadata": {
        "id": "aIPkx10e-YFV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}